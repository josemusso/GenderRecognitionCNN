{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import tarfile\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from scipy.misc import imresize\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cifar10mod import CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# importar etiquetas e imagenes\n",
    "filelist = glob.glob('UTKFace/*.jpg', recursive=True)\n",
    "\n",
    "imagenes = np.array([np.array(Image.open(fname)) for fname in filelist])\n",
    "\n",
    "\n",
    "def prepare_image(image, target_height=64, target_width=64):\n",
    "    image = imresize(image, (target_width, target_height))\n",
    "    return image\n",
    "\n",
    "# IMAGENES REESCALADAS A 64\n",
    "print('escalando imagenes...')\n",
    "imagenes_chicas=[]\n",
    "for i in range(imagenes.__len__()):\n",
    "    imagenes_chicas.append(prepare_image(imagenes[i]))\n",
    "imagenes_chicas = np.array(imagenes_chicas)\n",
    "#imgplot = plt.imshow(imagenes_chicas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edad=[]\n",
    "genero=[]\n",
    "raza=[]\n",
    "c=0\n",
    "for fname in filelist:\n",
    "    print(fname)\n",
    "    edad.append(int(fname.split('_')[0].split('/')[1]))\n",
    "    genero.append(int(fname.split('_')[1]))\n",
    "    raza.append(int(fname.split('_')[2]))\n",
    "    c+=1\n",
    "    print(c)\n",
    "edad=np.array(edad)\n",
    "genero=np.array(genero)\n",
    "raza=np.array(raza)\n",
    "#print(filelist.__len__())\n",
    "#print(edad[1])\n",
    "#print(genero[1])\n",
    "print(raza)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_blancos = np.zeros(117,dtype=int)\n",
    "h_negros = np.zeros(117,dtype=int)\n",
    "h_asiaticos = np.zeros(117,dtype=int)\n",
    "h_indios = np.zeros(117,dtype=int)\n",
    "h_otros = np.zeros(117,dtype=int)\n",
    "m_blancos = np.zeros(117,dtype=int)\n",
    "m_negros = np.zeros(117,dtype=int)\n",
    "m_asiaticos = np.zeros(117,dtype=int)\n",
    "m_indios = np.zeros(117,dtype=int)\n",
    "m_otros = np.zeros(117,dtype=int)\n",
    "for ident in range(genero.__len__()):\n",
    "    if genero[ident]==0:\n",
    "        if raza[ident]==0:\n",
    "            e=edad[ident]\n",
    "            h_blancos[e]+=1\n",
    "        elif raza[ident]==1:\n",
    "            e=edad[ident]\n",
    "            h_negros[e]+=1\n",
    "        elif raza[ident]==2:\n",
    "            e=edad[ident]\n",
    "            h_asiaticos[e]+=1\n",
    "        elif raza[ident]==3:\n",
    "            e=edad[ident]\n",
    "            h_indios[e]+=1   \n",
    "        elif raza[ident]==4:\n",
    "            e=edad[ident]\n",
    "            h_otros[e]+=1 \n",
    "    elif genero[ident]==1:\n",
    "        if raza[ident]==0:\n",
    "            e=edad[ident]\n",
    "            m_blancos[e]+=1\n",
    "        elif raza[ident]==1:\n",
    "            e=edad[ident]\n",
    "            m_negros[e]+=1\n",
    "        elif raza[ident]==2:\n",
    "            e=edad[ident]\n",
    "            m_asiaticos[e]+=1\n",
    "        elif raza[ident]==3:\n",
    "            e=edad[ident]\n",
    "            m_indios[e]+=1   \n",
    "        elif raza[ident]==4:\n",
    "            e=edad[ident]\n",
    "            m_otros[e]+=1 \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_blancos[0:40],label=\"blancos\")\n",
    "plt.plot(h_negros[0:40],label=\"negros\")\n",
    "plt.plot(h_asiaticos[0:40],label=\"asiaticos\")\n",
    "plt.plot(h_indios[0:40],label=\"indios\")\n",
    "plt.plot(h_otros[0:40],label=\"otros\")\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.xlabel('Edad')\n",
    "plt.title('Distribución de razas por edad en Hombres')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m_blancos[0:40],label=\"blanca\")\n",
    "plt.plot(m_negros[0:40],label=\"negra\")\n",
    "plt.plot(m_asiaticos[0:40],label=\"asiatica\")\n",
    "plt.plot(m_indios[0:40],label=\"india\")\n",
    "plt.plot(m_otros[0:40],label=\"otra\")\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.xlabel('Edad')\n",
    "plt.title('Distribución de razas por edad en Mujeres')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(h_blancos,label=\"blancos\")\n",
    "plt.plot(h_negroslabel=\"negros\")\n",
    "plt.plot(h_asiaticos,label=\"asiaticos\")\n",
    "plt.plot(h_indios,label=\"indios\")\n",
    "plt.plot(h_otros,label=\"otros\")\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.xlabel('Edad')\n",
    "plt.title('Distribución de razas por edad en Hombres')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(m_blancos,label=\"blancas\")\n",
    "plt.plot(m_negros,label=\"negras\")\n",
    "plt.plot(m_asiaticos,label=\"asiaticas\")\n",
    "plt.plot(m_indios,label=\"indias\")\n",
    "plt.plot(m_otros,label=\"otras\")\n",
    "plt.ylabel('Cantidad de datos')\n",
    "plt.xlabel('Edad')\n",
    "plt.title('Distribución de razas por edad en Mujeres')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardar datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos1 = {'data': imagenes_chicas[0:5541],\n",
    "         'labels':np.transpose([edad[0:5541],genero[0:5541],raza[0:5541]])\n",
    "          }\n",
    "\n",
    "filename1=\"datos1.pkl\"\n",
    "file1=open(filename1,'wb')\n",
    "pickle.dump(datos1,file1)\n",
    "file1.close()\n",
    "\n",
    "\n",
    "datos2 = {'data': imagenes_chicas[5541:11082],\n",
    "         'labels':np.transpose([edad[5541:11082],genero[5541:11082],raza[5541:11082]])\n",
    "          }\n",
    "\n",
    "filename2=\"datos2.pkl\"\n",
    "file2=open(filename2,'wb')\n",
    "pickle.dump(datos2,file2)\n",
    "file2.close()\n",
    "\n",
    "datos3 = {'data': imagenes_chicas[11082:16624],\n",
    "         'labels':np.transpose([edad[11082:16624],genero[11082:16624],raza[11082:16624]])\n",
    "          }\n",
    "\n",
    "filename3=\"datos3.pkl\"\n",
    "file3=open(filename3,'wb')\n",
    "pickle.dump(datos3,file3)\n",
    "file3.close()\n",
    "\n",
    "datos4 = {'data': imagenes_chicas[16624:22165],\n",
    "         'labels':np.transpose([edad[16624:22165],genero[16624:22165],raza[16624:22165]])\n",
    "          }\n",
    "filename4=\"datos4.pkl\"\n",
    "file4=open(filename4,'wb')\n",
    "pickle.dump(datos4,file4)\n",
    "file4.close()\n",
    "\n",
    "datos5 = {'data': imagenes_chicas[22165:27707],\n",
    "         'labels':np.transpose([edad[22165:27707],genero[22165:27707],raza[22165:27707]])\n",
    "          }\n",
    "filename5=\"datos5.pkl\"\n",
    "file5=open(filename5,'wb')\n",
    "pickle.dump(datos5,file5)\n",
    "file5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de función que importa los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esta parte del codigo hace de decifrador de datos, esto se hace ya \n",
    "que puede ser mucho consumo computacional hacer las imagenes a arreglos\n",
    "nuevamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[[1 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " ...\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "DIR_BINARIES = 'Datos/'\n",
    "\n",
    "\n",
    "def unpickle(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    dic = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    return dic\n",
    "\n",
    "\n",
    "def batch_to_bc01(batch):\n",
    "    ''' Converts CIFAR sample to bc01 tensor'''\n",
    "    return batch.reshape([-1, 3, 64, 64])                   # CAMBIADO SHAPE 32 POR 64\n",
    "\n",
    "\n",
    "def batch_to_b01c(batch):\n",
    "    ''' Converts CIFAR sample to b01c tensor'''\n",
    "    return batch_to_bc01(batch).transpose(0, 2, 3, 1)\n",
    "\n",
    "\n",
    "def labels_to_one_hot(labels):\n",
    "    ''' Converts list of integers to numpy 2D array with one-hot encoding'''\n",
    "    N = len(labels)\n",
    "    one_hot_labels = np.zeros([N, 2], dtype=int)# CAMBIADO 10 CLASES POR 2\n",
    "    one_hot_labels[np.arange(N),labels]= 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "class CIFAR10:\n",
    "    def __init__(self, batch_size=100, validation_proportion=0.1, test_proportion=0.1, augment_data=False):\n",
    "\n",
    "        # Training set\n",
    "        train_data_list = []\n",
    "        train_labels_list = []\n",
    "        for bi in range(1,6):\n",
    "            d = unpickle(DIR_BINARIES+'datos'+str(bi)+'.pkl')\n",
    "            train_data_list.append(d['data'])\n",
    "            train_labels_list.append(d['labels'])\n",
    "        self.train_data = np.concatenate(train_data_list, axis=0).astype(np.float32)\n",
    "        self.train_labels = np.concatenate(train_labels_list, axis=0).astype(np.uint8)\n",
    "\n",
    "        # Check set\n",
    "        check_proportion=validation_proportion + test_proportion\n",
    "        assert check_proportion > 0. and check_proportion < 1.\n",
    "        self.train_data, self.check_data, self.train_labels, self.check_labels = train_test_split(\n",
    "            self.train_data, self.train_labels, test_size=check_proportion, random_state=1)\n",
    "\n",
    "        # Validation set and Test set\n",
    "        self.test_data, self.validation_data, self.test_labels, self.validation_labels = train_test_split( \n",
    "            self.check_data, self.check_labels, test_size=validation_proportion, random_state=1)\n",
    "\n",
    "\n",
    "        # Normalize data\n",
    "        mean = self.train_data.mean(axis=0)\n",
    "        std = self.train_data.std(axis=0)\n",
    "        self.train_data = (self.train_data - mean) / std\n",
    "        self.validation_data = (self.validation_data - mean) / std\n",
    "        self.test_data = (self.test_data - mean) / std\n",
    "\n",
    "        # Converting to b01c and one-hot encoding\n",
    "        self.train_data = batch_to_b01c(self.train_data)\n",
    "        self.validation_data = batch_to_b01c(self.validation_data)\n",
    "        self.test_data = batch_to_b01c(self.test_data)\n",
    "        \n",
    "        self.train_labels_edad = np.transpose(self.train_labels)[0]\n",
    "        self.validation_labels_edad = np.transpose(self.validation_labels)[0]\n",
    "        self.test_labels_edad = np.transpose(self.test_labels)[0]\n",
    "        \n",
    "        self.train_labels_raza = np.transpose(self.train_labels)[2]\n",
    "        self.validation_labels_raza = np.transpose(self.validation_labels)[2]\n",
    "        self.test_labels_raza = np.transpose(self.test_labels)[2]\n",
    "        \n",
    "        self.train_labels_genero =np.transpose(self.train_labels)[1]\n",
    "        self.validation_labels_genero =np.transpose(self.validation_labels)[1]\n",
    "        self.test_labels_genero = np.transpose(self.test_labels)[1]\n",
    "        \n",
    "        self.train_labels_genero = labels_to_one_hot(self.train_labels_genero)\n",
    "        self.validation_labels_genero = labels_to_one_hot(self.validation_labels_genero)\n",
    "        self.test_labels_genero = labels_to_one_hot(self.test_labels_genero)\n",
    "\n",
    "        np.random.seed(seed=1)\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        # Batching & epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = len(self.train_labels_genero) // self.batch_size\n",
    "        self.current_batch = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def nextBatch(self):\n",
    "        ''' Returns a tuple with batch and batch index '''\n",
    "        start_idx = self.current_batch * self.batch_size\n",
    "        end_idx = start_idx + self.batch_size\n",
    "        batch_data = self.train_data[start_idx:end_idx]\n",
    "        batch_labels_genero = self.train_labels_genero[start_idx:end_idx]\n",
    "        batch_labels_raza = self.train_labels_raza[start_idx:end_idx]\n",
    "        batch_labels_edad = self.train_labels_edad[start_idx:end_idx]\n",
    "        batch_idx = self.current_batch\n",
    "\n",
    "        if self.augment_data:\n",
    "            if np.random.randint(0, 2) == 0:\n",
    "                batch_data = batch_data[:, :, ::-1, :]\n",
    "            batch_data += np.random.randn(self.batch_size, 1, 1, 3) * 0.05\n",
    "\n",
    "        # Update self.current_batch and self.current_epoch\n",
    "        self.current_batch = (self.current_batch + 1) % self.n_batches\n",
    "        if self.current_batch != batch_idx + 1:\n",
    "            self.current_epoch += 1\n",
    "\n",
    "            # shuffle training data\n",
    "            new_order = np.random.permutation(np.arange(len(self.train_labels_genero)))\n",
    "            self.train_data = self.train_data[new_order]\n",
    "            self.train_labels_genero = self.train_labels_genero[new_order]\n",
    "            self.train_labels_raza = self.train_labels_raza[new_order]\n",
    "            self.train_labels_edad = self.train_labels_edad[new_order]\n",
    "\n",
    "        return ((batch_data, batch_labels_genero, batch_labels_raza, batch_labels_edad), batch_idx)\n",
    "\n",
    "    def getEpoch(self):\n",
    "        return self.current_epoch\n",
    "\n",
    "    # TODO: refactor getTestSet and getValidationSet to avoid code replication\n",
    "    def getTestSet(self, asBatches=False):\n",
    "        if asBatches:\n",
    "            batches = []\n",
    "            for i in range(len(self.test_labels_genero) // self.batch_size):\n",
    "                start_idx = i * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                batch_data = self.test_data[start_idx:end_idx]\n",
    "                batch_labels_genero = self.test_labels_genero[start_idx:end_idx]\n",
    "                batch_labels_raza = self.test_labels_raza[start_idx:end_idx]\n",
    "                batch_labels_edad = self.test_labels_edad[start_idx:end_idx]\n",
    "\n",
    "                batches.append((batch_data, batch_labels_genero, batch_labels_raza, batch_labels_edad))\n",
    "            return batches\n",
    "        else:\n",
    "            return (self.test_data, self.test_labels_genero, self.test_labels_raza, self.test_labels_edad)\n",
    "\n",
    "    def getValidationSet(self, asBatches=False):\n",
    "        if asBatches:\n",
    "            batches = []\n",
    "            for i in range(len(self.validation_labels_genero) // self.batch_size):\n",
    "                start_idx = i * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                batch_data = self.validation_data[start_idx:end_idx]\n",
    "                batch_labels_genero = self.validation_labels_genero[start_idx:end_idx]\n",
    "                batch_labels_raza = self.validation_labels_raza[start_idx:end_idx]\n",
    "                batch_labels_edad = self.validation_labels_edad[start_idx:end_idx]\n",
    "\n",
    "                batches.append((batch_data, batch_labels_genero, batch_labels_raza, batch_labels_edad))\n",
    "            return batches\n",
    "        else:\n",
    "            return (self.validation_data, self.validation_labels_genero, self.validation_labels_raza, self.validation_labels_edad)\n",
    "\n",
    "    def shuffleValidation(self):\n",
    "        new_order = np.random.permutation(np.arange(len(self.validation_labels_genero)))\n",
    "        self.validation_labels_genero = self.validation_labels_genero[new_order]\n",
    "        self.validation_labels_raza = self.validation_labels_raza[new_order]\n",
    "        self.validation_labels_edad = self.validation_labels_edad[new_order]\n",
    "        self.validation_data = self.validation_data[new_order]\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_batch = 0\n",
    "        self.current_epoch = 0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cifar10 = CIFAR10(batch_size=1000)\n",
    "    while cifar10.getEpoch() < 2:\n",
    "        batch, batch_idx = cifar10.nextBatch()\n",
    "#         print(batch_idx, cifar10.n_batches, cifar10.getEpoch())\n",
    "    batch= cifar10.getTestSet(asBatches=True)\n",
    "#     print(len(batch))\n",
    "    for ba in batch:\n",
    "        data ,genero, raza,etnia=ba\n",
    "        print(genero)\n",
    "    data, genero, raza, edad = cifar10.getValidationSet()\n",
    "#     print(genero.sum(axis=0))\n",
    "    data, genero, raza, edad = cifar10.getTestSet()\n",
    "#     print(genero.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "batch_size = 1000\n",
    "cifar10 = CIFAR10(batch_size=batch_size, validation_proportion=0.1, test_proportion=0.1, augment_data=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model blocks\n",
    "def conv_layer(input_tensor, kernel_shape, layer_name):\n",
    "    # input_tensor b01c\n",
    "    # kernel_shape 01-in-out\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "                               initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    biases = tf.get_variable(\"biases\", [kernel_shape[3]],\n",
    "                             initializer=tf.constant_initializer(0.05))\n",
    "    \n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    \n",
    "    # Other options are to use He et. al init. for weights and 0.01 \n",
    "    # to init. biases.\n",
    "    conv = tf.nn.conv2d(input_tensor, weights, \n",
    "                       strides = [1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def fc_layer(input_tensor, weights_shape, layer_name):\n",
    "    # weights_shape in-out\n",
    "    weights = tf.get_variable(\"weights\", weights_shape,\n",
    "                              initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(\"biases\", [weights_shape[1]],\n",
    "                             initializer=tf.constant_initializer(0.0))\n",
    "    tf.summary.histogram(layer_name + \"/weights\", weights)\n",
    "    tf.summary.histogram(layer_name + \"/biases\", biases)\n",
    "    mult_out = tf.matmul(input_tensor, weights)\n",
    "    return tf.nn.relu(mult_out+biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Elija aquí el directorio donde guardará los registros de Tensorboard\"\"\"\n",
    "PARENT_DIR = './summaries/'\n",
    "SUMMARIES_DIR = PARENT_DIR + 'conv_2_layer_with_dropout'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-0681f17da443>:82: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "use_convnet = True\n",
    "n_conv_layers = 2\n",
    "\n",
    "n_filters_convs = [32, 64, 128]\n",
    "\n",
    "model_input = tf.placeholder(tf.float32, name='model_input', \n",
    "                             shape=(batch_size, 64, 64, 3))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='dropout_prob', shape=())\n",
    "\n",
    "target = tf.placeholder(tf.float32, name='target', shape=(batch_size, 2))\n",
    "\n",
    "if use_convnet:\n",
    "    layer_input = model_input\n",
    "    previous_n_feature_maps = 3\n",
    "    for layer_index in range(n_conv_layers):\n",
    "      layer_name = 'conv%d' % layer_index\n",
    "      with tf.variable_scope(layer_name):\n",
    "        conv_out = conv_layer(\n",
    "            layer_input, \n",
    "            [5, 5, previous_n_feature_maps, n_filters_convs[layer_index]], \n",
    "            layer_name)\n",
    "      if layer_index == 0:\n",
    "        with tf.variable_scope(layer_name, reuse=True):\n",
    "          conv1_filters = tf.get_variable(\"weights\")\n",
    "          tf.summary.image(\n",
    "              'conv1_filters',\n",
    "              tf.transpose(conv1_filters, perm=[3, 0, 1, 2]),\n",
    "              max_outputs=n_filters_convs[layer_index]\n",
    "          )\n",
    "      previous_n_feature_maps = n_filters_convs[layer_index]\n",
    "      pool_out = tf.nn.max_pool(\n",
    "          conv_out, \n",
    "          ksize=[1, 2, 2, 1],\n",
    "          strides=[1, 2, 2, 1],\n",
    "          padding='SAME',\n",
    "          name='pool%d' % layer_index)\n",
    "      layer_input = pool_out\n",
    "     \n",
    "\n",
    "    fc_input = tf.layers.flatten(pool_out, name='fc_input')\n",
    "\n",
    "    feature_map_height = int(64 / (2**n_conv_layers))\n",
    "    \n",
    "    # First fully connected layer\n",
    "    layer_name = 'fc1'\n",
    "    with tf.variable_scope(layer_name):\n",
    "        fc1_out = fc_layer(\n",
    "            fc_input, \n",
    "            [(feature_map_height**2)*previous_n_feature_maps, 50], \n",
    "            layer_name)\n",
    "\n",
    "    fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "    # Second fully connected layer\n",
    "    layer_name = 'fc2'\n",
    "    with tf.variable_scope(layer_name):\n",
    "        fc2_out = fc_layer(fc1_out_drop, [50, 2], layer_name)\n",
    "    model_output = fc2_out\n",
    "        \n",
    "else:\n",
    "    # Reshape tensor to MLP\n",
    "    first_layer_input = tf.reshape(model_input, [-1,3072], name='first_layer_input')\n",
    "\n",
    "    # First layer\n",
    "    layer_name = 'fc1'\n",
    "    with tf.variable_scope(layer_name):\n",
    "        fc1_out = fc_layer(first_layer_input, [3072, 100], layer_name)\n",
    "\n",
    "    fc1_out_drop = tf.nn.dropout(fc1_out, keep_prob)\n",
    "\n",
    "    # Second layer\n",
    "    layer_name = 'fc2'\n",
    "    with tf.variable_scope(layer_name):\n",
    "        fc2_out = fc_layer(fc1_out_drop, [100, 10], layer_name)\n",
    "    model_output = fc2_out\n",
    "\n",
    "with tf.name_scope('loss_function'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=model_output, labels=target,\n",
    "                                           name='cross_entropy'))\n",
    "    xentropy_summary = tf.summary.scalar('cross_entropy', cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.0005)\n",
    "    grads_vars = optimizer.compute_gradients(cross_entropy)\n",
    "    optimizer.apply_gradients(grads_vars)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# Metrics\n",
    "correct_prediction = tf.equal(tf.argmax(model_output, 1),\n",
    "                             tf.argmax(target, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful training functions\n",
    "def validate():\n",
    "    cifar10.shuffleValidation()\n",
    "    batches = cifar10.getValidationSet(asBatches=True)\n",
    "    accs = []\n",
    "    xent_vals = []\n",
    "    for batch in batches:\n",
    "        data, genero, raza, edad = batch\n",
    "        acc, xentropy_val = sess.run((accuracy, cross_entropy),\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: genero,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "        xent_vals.append(xentropy_val)\n",
    "    mean_xent = np.array(xent_vals).mean()    \n",
    "    mean_acc = np.array(accs).mean()\n",
    "    summary = sess.run(\n",
    "        merged,\n",
    "        feed_dict={\n",
    "            model_input: data,\n",
    "            target: genero,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "    return summary, mean_acc, mean_xent\n",
    "def test():\n",
    "    batches = cifar10.getTestSet(asBatches=True)\n",
    "    accs = []\n",
    "    for batch in batches:\n",
    "        data, genero, raza, edad = batch\n",
    "        acc = sess.run(accuracy,\n",
    "                       feed_dict={\n",
    "                model_input: data,\n",
    "                target: genero,\n",
    "                keep_prob: 1.0\n",
    "            })\n",
    "        accs.append(acc)\n",
    "    mean_acc = np.array(accs).mean()\n",
    "    return mean_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensorboard writers\n",
    "train_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/train',\n",
    "                                     sess.graph)\n",
    "validation_writer = tf.summary.FileWriter(SUMMARIES_DIR+'/validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables\n",
      "conv0/weights:0\n",
      "conv0/biases:0\n",
      "conv1/weights:0\n",
      "conv1/biases:0\n",
      "fc1/weights:0\n",
      "fc1/biases:0\n",
      "fc2/weights:0\n",
      "fc2/biases:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "cifar10.reset()\n",
    "print(\"Trainable variables\")\n",
    "for n in tf.trainable_variables():\n",
    "    print(n.name)\n",
    "\n",
    "if use_convnet:\n",
    "    epochs = 30 #DEBERIAMOS SUBIR ESTE VALOR\n",
    "else:\n",
    "    epochs = 50\n",
    "    \n",
    "t_i = time.time()\n",
    "n_batches = cifar10.n_batches\n",
    "val_acc_vals = []\n",
    "test_acc_vals = []\n",
    "while cifar10.getEpoch() < epochs:\n",
    "    epoch = cifar10.getEpoch()\n",
    "    batch, batch_idx = cifar10.nextBatch()\n",
    "    batch_data = batch[0]\n",
    "    batch_genero = batch[1]\n",
    "    batch_raza = batch[2]\n",
    "    batch_edad = batch[3]\n",
    "    \n",
    "    # just a training iteration\n",
    "    _ = sess.run(train_step,\n",
    "                feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_genero,\n",
    "            keep_prob: 0.8 ###### Modifique el dropout aqui y solo aqui. #####\n",
    "        })\n",
    "    \n",
    "    step = batch_idx+epoch*n_batches\n",
    "    \n",
    "    # Write training summary\n",
    "    if step%50==0:\n",
    "        summary = sess.run(learning_summaries,\n",
    "                          feed_dict={\n",
    "                model_input: batch_data,\n",
    "                target: batch_genero,\n",
    "                keep_prob: 1.0 # set to 1.0 at inference time\n",
    "            })\n",
    "        train_writer.add_summary(summary, step)\n",
    "        \n",
    "    # gradient (by layer) statistics over last training batch & validation summary\n",
    "    if batch_idx==0:\n",
    "        loss, acc, grads = sess.run((cross_entropy, accuracy, grads_vars), \n",
    "                      feed_dict={\n",
    "            model_input: batch_data,\n",
    "            target: batch_genero,\n",
    "            keep_prob: 1.0\n",
    "        })\n",
    "        \n",
    "        summary, validation_accuracy, validation_loss = validate()\n",
    "        validation_writer.add_summary(summary, step)\n",
    "        print('[Epoch %d, it %d] Training acc. %.3f, loss %.3f. \\\n",
    "Valid. acc. %.3f, loss %.3f' % (\n",
    "            epoch,\n",
    "            step,\n",
    "            acc,\n",
    "            loss,\n",
    "            validation_accuracy,\n",
    "            validation_loss\n",
    "        ))\n",
    "        val_acc_vals.append(validation_accuracy)\n",
    "        test_accuracy = test()\n",
    "        test_acc_vals.append(test_accuracy)\n",
    "        print(\"Time elapsed %.2f minutes\" % ((time.time()-t_i)/60.0))\n",
    "train_writer.flush()\n",
    "validation_writer.flush()\n",
    "\n",
    "val_acc_vals = np.array(val_acc_vals)\n",
    "test_acc_vals = np.array(test_acc_vals)\n",
    "best_epoch = np.argmax(val_acc_vals)\n",
    "test_acc_at_best = test_acc_vals[best_epoch]\n",
    "print('*'*30)\n",
    "print(\"Testing set accuracy @ epoch %d (best validation acc): %.4f\" % (best_epoch, test_acc_at_best))\n",
    "print('*'*30)\n",
    "\n",
    "print('TP = ', np.count_nonzero(TP_cast == 1))\n",
    "print('FP = ', np.count_nonzero(FP_cast == 1))\n",
    "print('FP = ', np.count_nonzero(FN_cast == 1))\n",
    "print('TN = ', np.count_nonzero(TN_cast == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
